{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Translation\n",
    "In this project, I implement a seq2seq Neural Machine Translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import util\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all the data and save it\n",
    "Make the characters in each sentence into lower case, and add the `<EOS>` tag in the end of each target senquence to indicate the end of a target sentence. In addition, the text would be converted into the corresponding id (one-hot encoding) and saved for the further process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "\n",
    "util.preprocess_and_save_data(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "This will check to make sure you have the correct version of TensorFlow and access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:\n",
    "- `model_inputs`\n",
    "- `process_decoder_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### Input\n",
    "Implement the `model_inputs()` function to create TF Placeholders for the Neural Network. It should create the following placeholders:\n",
    "\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "- Targets placeholder with rank 2.\n",
    "- Learning rate placeholder with rank 0.\n",
    "- Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "- Target sequence length placeholder named \"target_sequence_length\" with rank 1\n",
    "- Max target sequence length tensor named \"max_target_len\" getting its value from applying tf.reduce_max on the target_sequence_length placeholder. Rank 0.\n",
    "- Source sequence length placeholder named \"source_sequence_length\" with rank 1\n",
    "\n",
    "Return the placeholders in the following the tuple (input, targets, learning rate, keep probability, target sequence length, max target sequence length, source sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "\n",
    "    # placeholder for input sequence data\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"input\")\n",
    "    # placeholder for target sequence data\n",
    "    targets = tf.placeholder(tf.int32, shape=[None, None], name=\"target\")\n",
    "    # placeholder for the learning rate of optimization process\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "    # placeholder for the keep probability of dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, shape=[], name=\"keep_prob\")\n",
    "    # placeholder for lenght of the current target sequences\n",
    "    target_sequence_length = tf.placeholder(tf.int32, shape=(None,), name=\"target_sequence_length\")\n",
    "    # the maximum length of the current target sequences\n",
    "    max_target_len = tf.reduce_max(target_sequence_length, name='max_target_len')   \n",
    "    # placeholder for lenght of the current target sequences\n",
    "    source_sequence_length = tf.placeholder(tf.int32, shape=(None,), name=\"source_sequence_length\")\n",
    "    # a variable for global step\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_prob, target_sequence_length, max_target_len, source_sequence_length, global_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "Implement `process_decoder_input` by removing the last word id from each batch in `target_data` and concat the GO ID to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "        \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Implement `encoding_layer()` to create a Encoder RNN layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer using lstm cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # create the input sequence embedding \n",
    "    embed_inputs = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                                   vocab_size=source_vocab_size, \n",
    "                                                   embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    # build lstm cell with dropout\n",
    "    def build_lstm(num_units, keep_prob):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_units, \n",
    "                                            initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        \n",
    "        dropout = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "                                                output_keep_prob=keep_prob)\n",
    "        \n",
    "        return dropout\n",
    "    \n",
    "    # build a multilayer LSTM cell\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [build_lstm(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    # Build a bidirectional RNN\n",
    "    fw_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [build_lstm(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    bw_cell = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [build_lstm(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                                    fw_cell,\n",
    "                                                    bw_cell,\n",
    "                                                    embed_inputs,\n",
    "                                                    dtype=tf.float32,\n",
    "                                                    sequence_length=source_sequence_length)\n",
    "\n",
    "    return tf.concat(bi_outputs, -1), bi_state[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Create a training decoding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_train(initial_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    \"\"\"\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input, \n",
    "                                               sequence_length=target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, \n",
    "                                              helper=helper, \n",
    "                                              initial_state=initial_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True,\n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Create inference decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(initial_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    \"\"\"\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=dec_embeddings, \n",
    "                                             start_tokens=tf.fill([batch_size], start_of_sequence_id), \n",
    "                                             end_token=end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, \n",
    "                                              helper=helper, \n",
    "                                              initial_state=initial_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                impute_finished=True, \n",
    "                                                maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoding Layer\n",
    "Implement `decoding_layer()` to create a Decoder RNN layer.\n",
    "\n",
    "* Embed the target sequences\n",
    "* Construct the decoder LSTM cell (just like you constructed the encoder cell above)\n",
    "* Create an output layer to map the outputs of the decoder to the elements of our vocabulary\n",
    "* Use the your `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)` function to get the training logits.\n",
    "* Use your `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, max_target_sequence_length, vocab_size, output_layer, batch_size, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: You'll need to use [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state, encoder_output,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size], \n",
    "                                                   minval=-1, \n",
    "                                                   maxval=1))\n",
    "    \n",
    "    dec_embed_inputs = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "    \n",
    "    \n",
    "    def build_lstm(num_units, keep_prob):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_units, \n",
    "                                            initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        \n",
    "        dropout = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "                                                output_keep_prob=keep_prob)\n",
    "        \n",
    "        return dropout\n",
    "    \n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [build_lstm(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    output_layer = Dense(target_vocab_size, \n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1),\n",
    "                         use_bias=False)\n",
    "\n",
    "    # Use attention mechanism for decoder\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                                            num_units=rnn_size,\n",
    "                                            memory=encoder_output,\n",
    "                                            memory_sequence_length=source_sequence_length)\n",
    "    \n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(stacked_lstm,\n",
    "                                                   attention_mechanism,\n",
    "                                                   attention_layer_size=rnn_size)\n",
    "    \n",
    "   \n",
    "    initial_state = dec_cell.zero_state(batch_size = batch_size, dtype=tf.float32 )\n",
    "    initial_state = initial_state.clone(cell_state = encoder_state)\n",
    "    \n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        train_outputs = decoding_layer_train(initial_state, dec_cell, dec_embed_inputs, \n",
    "                                             target_sequence_length, max_target_sequence_length, \n",
    "                                             output_layer, keep_prob)\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_outputs = decoding_layer_infer(initial_state, dec_cell, dec_embeddings, target_vocab_to_int['<GO>'],\n",
    "                                             target_vocab_to_int['<EOS>'], max_target_sequence_length,\n",
    "                                             target_vocab_size, output_layer, batch_size, keep_prob)\n",
    "    \n",
    "    return train_outputs, infer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "\n",
    "- Encode the input using your `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,  source_sequence_length, source_vocab_size, encoding_embedding_size)`.\n",
    "- Process target data using your `process_decoder_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Decode the encoded input using your `decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    encoder_output, encoder_state = encoding_layer(input_data, rnn_size, num_layers, keep_prob, \n",
    "                                       source_sequence_length, source_vocab_size, \n",
    "                                       enc_embedding_size)\n",
    "\n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    train_outputs, infer_outputs = decoding_layer(dec_input, encoder_state, encoder_output,\n",
    "                                                   target_sequence_length, max_target_sentence_length,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                                                   batch_size, keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return train_outputs, infer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `num_layers` to the number of layers.\n",
    "- Set `encoding_embedding_size` to the size of the embedding for the encoder.\n",
    "- Set `decoding_embedding_size` to the size of the embedding for the decoder.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `keep_probability` to the Dropout keep probability\n",
    "- Set `display_step` to state how many steps between each debug output statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 64\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 64\n",
    "decoding_embedding_size = 64\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.8\n",
    "display_step = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = util.load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in target_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length, global_step = model_inputs()\n",
    "\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "    # make a copy of train_logits\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    # make a copy of inference_logits\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # masks are created to facilitate the calculation of the loss.\n",
    "    # We don't want to calculate the loss for the padding in a sentence.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping is applied to mitigate the issue of exploding gradients\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for test data 1379,1379\n"
     ]
    }
   ],
   "source": [
    "# split original data into training and test data \n",
    "source_train, source_test, target_train, target_test = train_test_split(source_int_text, \n",
    "                                                                        target_int_text, test_size=0.01, random_state=42)\n",
    "\n",
    "print(\"The size for test data {},{}\".format(len(source_test), len(target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch and pad the source and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to training and validation sets\n",
    "train_source = source_train[batch_size:]\n",
    "train_target = target_train[batch_size:]\n",
    "valid_source = source_train[:batch_size]\n",
    "valid_target = target_train[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(util.get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for training data 136354,136354\n",
      "The size for vallidation data 128,128\n"
     ]
    }
   ],
   "source": [
    "print(\"The size for training data {},{}\".format(len(train_source), len(train_target)))\n",
    "print(\"The size for vallidation data {},{}\".format(len(valid_source), len(valid_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch  500/1077 - Loss: 0.9883\n",
      "Epoch   0 Batch 1000/1077 - Loss: 0.6171\n",
      "Epoch   1 Batch  500/1077 - Loss: 0.4763\n",
      "Epoch   1 Batch 1000/1077 - Loss: 0.2796\n",
      "Epoch   2 Batch  500/1077 - Loss: 0.1515\n",
      "Epoch   2 Batch 1000/1077 - Loss: 0.1122\n",
      "Epoch   3 Batch  500/1077 - Loss: 0.0798\n",
      "Epoch   3 Batch 1000/1077 - Loss: 0.0709\n",
      "Epoch   4 Batch  500/1077 - Loss: 0.0626\n",
      "Epoch   4 Batch 1000/1077 - Loss: 0.0540\n",
      "Epoch   5 Batch  500/1077 - Loss: 0.0407\n",
      "Epoch   5 Batch 1000/1077 - Loss: 0.0471\n",
      "Epoch   6 Batch  500/1077 - Loss: 0.0388\n",
      "Epoch   6 Batch 1000/1077 - Loss: 0.0438\n",
      "Epoch   7 Batch  500/1077 - Loss: 0.0347\n",
      "Epoch   7 Batch 1000/1077 - Loss: 0.0344\n",
      "Epoch   8 Batch  500/1077 - Loss: 0.0298\n",
      "Epoch   8 Batch 1000/1077 - Loss: 0.0293\n",
      "Epoch   9 Batch  500/1077 - Loss: 0.0271\n",
      "Epoch   9 Batch 1000/1077 - Loss: 0.0339\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "count = 0                                                                                            \n",
    "# train the model in mini-batch\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # mini-batch training\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                util.get_batches(train_source, train_target, batch_size,\n",
    "                                source_vocab_to_int['<PAD>'],\n",
    "                                target_vocab_to_int['<PAD>'])):\n",
    "            count += 1 \n",
    "            # execute the train_op and cost operations to optimize the model and retrieve the loss.\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "            loss_list.append((count, loss))\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size,  loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the blue score (training and inference) during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAW3iEjAKLhFRAJGwS0iEjD/H28u4ju7Wu+2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21a1dd96470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*zip(*loss_list), label=\"Train Loss\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "util.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = util.load_preprocess()\n",
    "load_path = util.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model\n",
    "Finally, the trained neural machine translation model is evaluated using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [60, 43, 186, 8, 194, 21, 160, 128, 114, 43, 186, 153, 65, 200, 62, 0, 0]\n",
      "  English Words: california is sometimes dry during may , and it is sometimes wonderful in february .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [194, 311, 42, 119, 209, 196, 156, 70, 136, 289, 193, 311, 42, 189, 314, 103, 1]\n",
      "  French Words: california est parfois sec au mois de mai , et il est parfois moins aimé .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [12, 311, 42, 119, 209, 196, 156, 70, 136, 289, 193, 311, 42, 275, 197, 85, 103, 1, 0, 0, 0]\n",
      "  French Words: californie est parfois sec au mois de mai , et il est parfois merveilleux en février .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [133, 43, 186, 129, 194, 97, 160, 128, 114, 43, 199, 195, 65, 132, 62, 0, 0]\n",
      "  English Words: paris is sometimes beautiful during summer , and it is usually snowy in fall .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [129, 311, 42, 72, 96, 228, 77, 136, 289, 193, 311, 217, 45, 197, 14, 103, 1]\n",
      "  French Words: paris est parfois belle pendant l' été , et il est généralement enneigée en septembre .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [129, 311, 42, 72, 96, 228, 77, 136, 289, 193, 311, 217, 45, 231, 228, 236, 103, 1, 0, 0]\n",
      "  French Words: paris est parfois belle pendant l' été , et il est généralement enneigée à l' automne .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shane\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Shane\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\Shane\\Anaconda2\\envs\\tensorflow-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "  Word Ids:      [89, 227, 101, 77, 43, 81, 112, 160, 201, 49, 227, 101, 43, 81, 24, 62, 0]\n",
      "  English Words: your least liked fruit is the pear , but their least liked is the banana .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [260, 189, 314, 33, 311, 349, 24, 136, 315, 140, 189, 314, 311, 349, 134, 103, 1]\n",
      "  French Words: votre moins aimé fruit est la poire , mais leur moins aimé est la mangue .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [260, 189, 314, 33, 311, 349, 24, 136, 315, 140, 189, 314, 311, 349, 343, 103, 1, 0, 0, 0]\n",
      "  French Words: votre moins aimé fruit est la poire , mais leur moins aimé est la banane .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [58, 175, 43, 7, 194, 12, 160, 201, 114, 43, 186, 82, 65, 132, 62, 0, 0]\n",
      "  English Words: new jersey is quiet during january , but it is sometimes rainy in fall .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [286, 255, 311, 206, 197, 46, 136, 315, 193, 311, 42, 348, 197, 85, 103, 1, 0]\n",
      "  French Words: new jersey est calme en janvier , mais il est parfois pluvieux en février .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [286, 255, 311, 206, 197, 46, 136, 315, 193, 311, 42, 348, 231, 228, 236, 103, 1, 0, 0, 0]\n",
      "  French Words: new jersey est calme en janvier , mais il est parfois pluvieux à l' automne .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [182, 43, 4, 8, 194, 118, 160, 201, 114, 43, 199, 76, 65, 200, 62, 0, 0]\n",
      "  English Words: china is never dry during march , but it is usually mild in february .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [337, 311, 79, 119, 197, 93, 136, 315, 193, 311, 217, 303, 197, 85, 103, 1, 0]\n",
      "  French Words: chine est jamais sec en mars , mais il est généralement doux en février .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [337, 311, 79, 231, 119, 209, 196, 156, 93, 136, 315, 193, 311, 217, 303, 197, 85, 103, 1, 0]\n",
      "  French Words: chine est jamais à sec au mois de mars , mais il est généralement doux en février .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [60, 43, 199, 123, 194, 214, 160, 201, 114, 43, 4, 82, 65, 12, 62, 0, 0]\n",
      "  English Words: california is usually busy during spring , but it is never rainy in january .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [12, 311, 217, 172, 209, 159, 136, 315, 79, 240, 29, 197, 46, 103, 1, 0, 0]\n",
      "  French Words: californie est généralement occupé au printemps , mais jamais des pluies en janvier .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [12, 311, 217, 172, 209, 159, 136, 315, 79, 240, 29, 197, 46, 103, 1, 0, 0, 0, 0, 0]\n",
      "  French Words: californie est généralement occupé au printemps , mais jamais des pluies en janvier .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [81, 155, 43, 146, 37, 85, 77, 160, 201, 81, 113, 43, 18, 37, 85, 62, 0]\n",
      "  English Words: the apple is my most loved fruit , but the grape is his most loved .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [349, 230, 311, 182, 33, 182, 98, 187, 31, 136, 315, 182, 88, 311, 182, 98, 314]\n",
      "  French Words: la pomme est le fruit le plus mon cher , mais le raisin est le plus aimé\n",
      "\n",
      "Target\n",
      "  Word Ids:      [349, 230, 311, 182, 33, 182, 98, 187, 31, 136, 315, 182, 88, 311, 182, 98, 314, 103, 1]\n",
      "  French Words: la pomme est le fruit le plus mon cher , mais le raisin est le plus aimé .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [35, 121, 38, 160, 147, 160, 128, 181, 62, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  English Words: i dislike pears , grapefruit , and bananas .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [52, 100, 66, 203, 62, 136, 203, 214, 289, 203, 204, 103, 1, 0, 0, 0, 0]\n",
      "  French Words: je n'aime pas les poires , les pamplemousses et les bananes .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [52, 100, 66, 203, 62, 136, 203, 214, 289, 203, 204, 103, 1, 0, 0, 0, 0, 0, 0]\n",
      "  French Words: je n'aime pas les poires , les pamplemousses et les bananes .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [10, 94, 208, 160, 149, 160, 128, 148, 62, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  English Words: he likes mangoes , grapes , and lemons .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [193, 152, 203, 110, 136, 203, 114, 289, 203, 89, 103, 1, 0, 0, 0, 0, 0]\n",
      "  French Words: il aime les mangues , les raisins et les citrons .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [193, 152, 203, 110, 136, 203, 114, 289, 203, 89, 103, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  French Words: il aime les mangues , les raisins et les citrons .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [40, 43, 4, 129, 194, 21, 160, 201, 114, 43, 180, 65, 135, 62, 0, 0, 0]\n",
      "  English Words: france is never beautiful during may , but it is hot in august .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [349, 284, 311, 79, 72, 209, 196, 156, 70, 136, 315, 193, 311, 265, 197, 238, 103]\n",
      "  French Words: la france est jamais belle au mois de mai , mais il est chaud en août .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [349, 284, 311, 79, 72, 209, 196, 156, 70, 136, 315, 193, 311, 265, 197, 238, 103, 1, 0, 0]\n",
      "  French Words: la france est jamais belle au mois de mai , mais il est chaud en août .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model and restore the saved variables\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #\n",
    "    for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                util.get_batches(source_test, target_test, batch_size,\n",
    "                                source_vocab_to_int['<PAD>'],\n",
    "                                target_vocab_to_int['<PAD>'])):\n",
    "        \n",
    "        translate_logits = sess.run(logits, {input_data: source_batch,\n",
    "                               target_sequence_length: sources_lengths,\n",
    "                               source_sequence_length: targets_lengths,\n",
    "                               keep_prob: 1.0})\n",
    "        \n",
    "        print('Input')\n",
    "        print('  Word Ids:      {}'.format([i for i in source_batch[0]]))\n",
    "        print('  English Words: {}'.format(\" \".join([source_int_to_vocab[i] for i in source_batch[0] \n",
    "                                           if i not in [0, 1]])))\n",
    "\n",
    "        print('\\nPrediction')\n",
    "        print('  Word Ids:      {}'.format([i for i in translate_logits[0]]))\n",
    "        print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in translate_logits[0] \n",
    "                                                   if i not in [0, 1]])))\n",
    "        \n",
    "        print('\\nTarget')\n",
    "        print('  Word Ids:      {}'.format([i for i in target_batch[0]]))\n",
    "        print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in target_batch[0]\n",
    "                                                  if i not in [0, 1]])))\n",
    "        print(\"\\n------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        test_acc = util.get_bleu(target_batch, translate_logits)\n",
    "        test_acc_list.append(test_acc)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the test data is 0.7237481306067661\n"
     ]
    }
   ],
   "source": [
    "print(\"The BLEU score for the test data is {}\".format(np.mean(test_acc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Language Translation\n",
    "This is the implementation of baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import util\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all the data and save it\n",
    "Make the characters in each sentence into lower case, and add the `<EOS>` tag in the end of each target senquence to indicate the end of a target sentence. In addition, the text would be converted into the corresponding id (one-hot encoding) and saved for the further process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_path = 'data/small_vocab_en'\n",
    "target_path = 'data/small_vocab_fr'\n",
    "\n",
    "util.preprocess_and_save_data(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU\n",
    "This will check to make sure you have the correct version of TensorFlow and access to a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.1'), 'Please use TensorFlow version 1.1 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a Sequence-to-Sequence model by implementing the following functions below:\n",
    "- `model_inputs`\n",
    "- `process_decoder_input`\n",
    "- `encoding_layer`\n",
    "- `decoding_layer_train`\n",
    "- `decoding_layer_infer`\n",
    "- `decoding_layer`\n",
    "- `seq2seq_model`\n",
    "\n",
    "### Input\n",
    "Implement the `model_inputs()` function to create TF Placeholders for the Neural Network. It should create the following placeholders:\n",
    "\n",
    "- Input text placeholder named \"input\" using the TF Placeholder name parameter with rank 2.\n",
    "- Targets placeholder with rank 2.\n",
    "- Learning rate placeholder with rank 0.\n",
    "- Keep probability placeholder named \"keep_prob\" using the TF Placeholder name parameter with rank 0.\n",
    "- Target sequence length placeholder named \"target_sequence_length\" with rank 1\n",
    "- Max target sequence length tensor named \"max_target_len\" getting its value from applying tf.reduce_max on the target_sequence_length placeholder. Rank 0.\n",
    "- Source sequence length placeholder named \"source_sequence_length\" with rank 1\n",
    "\n",
    "Return the placeholders in the following the tuple (input, targets, learning rate, keep probability, target sequence length, max target sequence length, source sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, learning rate, and lengths of source and target sequences.\n",
    "    :return: Tuple (input, targets, learning rate, keep probability, target sequence length,\n",
    "    max target sequence length, source sequence length)\n",
    "    \"\"\"\n",
    "\n",
    "    # placeholder for input sequence data\n",
    "    inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"input\")\n",
    "    # placeholder for target sequence data\n",
    "    targets = tf.placeholder(tf.int32, shape=[None, None], name=\"target\")\n",
    "    # placeholder for the learning rate of optimization process\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
    "    # placeholder for the keep probability of dropout\n",
    "    keep_prob = tf.placeholder(tf.float32, shape=[], name=\"keep_prob\")\n",
    "    # placeholder for lenght of the current target sequences\n",
    "    target_sequence_length = tf.placeholder(tf.int32, shape=(None,), name=\"target_sequence_length\")\n",
    "    # the maximum length of the current target sequences\n",
    "    max_target_len = tf.reduce_max(target_sequence_length, name='max_target_len')   \n",
    "    # placeholder for lenght of the current target sequences\n",
    "    source_sequence_length = tf.placeholder(tf.int32, shape=(None,), name=\"source_sequence_length\")\n",
    "    # a variable for global step\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "    return inputs, targets, learning_rate, keep_prob, target_sequence_length, max_target_len, source_sequence_length, global_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Decoder Input\n",
    "Implement `process_decoder_input` by removing the last word id from each batch in `target_data` and concat the GO ID to the begining of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "    Preprocess target data for encoding\n",
    "    return: Preprocessed target data\n",
    "    \"\"\"\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), ending], 1)\n",
    "        \n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Implement `encoding_layer()` to create a Encoder RNN layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create encoding layer using lstm cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # create the input sequence embedding \n",
    "    embed_inputs = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                                   vocab_size=source_vocab_size, \n",
    "                                                   embed_dim=encoding_embedding_size)\n",
    "    \n",
    "    # build lstm cell with dropout\n",
    "    def build_lstm(num_units, keep_prob):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_units, \n",
    "                                            initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        \n",
    "        dropout = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "                                                output_keep_prob=keep_prob)\n",
    "        \n",
    "        return dropout\n",
    "    \n",
    "    # build a multilayer RNN cell\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [build_lstm(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    # build a multilayer RNN\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(stacked_lstm, \n",
    "                                             embed_inputs, \n",
    "                                             sequence_length=source_sequence_length, \n",
    "                                             dtype=tf.float32)\n",
    "    \n",
    "    return outputs, final_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Training\n",
    "Create a training decoding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def decoding_layer_train(encoder_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for training\n",
    "    \"\"\"\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input, \n",
    "                                               sequence_length=target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, \n",
    "                                              helper=helper, \n",
    "                                              initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True,\n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding - Inference\n",
    "Create inference decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a decoding layer for inference\n",
    "    \"\"\"\n",
    "\n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=dec_embeddings, \n",
    "                                             start_tokens=tf.fill([batch_size], start_of_sequence_id), \n",
    "                                             end_token=end_of_sequence_id)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell, \n",
    "                                              helper=helper, \n",
    "                                              initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                impute_finished=True, \n",
    "                                                maximum_iterations=max_target_sequence_length)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoding Layer\n",
    "Implement `decoding_layer()` to create a Decoder RNN layer.\n",
    "\n",
    "* Embed the target sequences\n",
    "* Construct the decoder LSTM cell (just like you constructed the encoder cell above)\n",
    "* Create an output layer to map the outputs of the decoder to the elements of our vocabulary\n",
    "* Use the your `decoding_layer_train(encoder_state, dec_cell, dec_embed_input, target_sequence_length, max_target_sequence_length, output_layer, keep_prob)` function to get the training logits.\n",
    "* Use your `decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id, max_target_sequence_length, vocab_size, output_layer, batch_size, keep_prob)` function to get the inference logits.\n",
    "\n",
    "Note: You'll need to use [tf.variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope) to share variables between training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, encoder_state,\n",
    "                   target_sequence_length, max_target_sequence_length,\n",
    "                   rnn_size,\n",
    "                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, decoding_embedding_size):\n",
    "    \"\"\"\n",
    "    Create decoding layer\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size], \n",
    "                                                   minval=-1, \n",
    "                                                   maxval=1))\n",
    "    \n",
    "    dec_embed_inputs = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    \n",
    "    \n",
    "    def build_lstm(num_units, keep_prob):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(num_units, \n",
    "                                            initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        \n",
    "        dropout = tf.contrib.rnn.DropoutWrapper(lstm_cell, \n",
    "                                                output_keep_prob=keep_prob)\n",
    "        \n",
    "        return dropout\n",
    "    \n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "                    [build_lstm(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    output_layer = Dense(target_vocab_size, \n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1),\n",
    "                         use_bias=False)\n",
    "\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        train_outputs = decoding_layer_train(encoder_state, stacked_lstm, dec_embed_inputs, \n",
    "                                             target_sequence_length, max_target_sequence_length, \n",
    "                                             output_layer, keep_prob)\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_outputs = decoding_layer_infer(encoder_state, stacked_lstm, dec_embeddings, target_vocab_to_int['<GO>'],\n",
    "                                             target_vocab_to_int['<EOS>'], max_target_sequence_length,\n",
    "                                             target_vocab_size, output_layer, batch_size, keep_prob)\n",
    "    \n",
    "    return train_outputs, infer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "\n",
    "- Encode the input using your `encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob,  source_sequence_length, source_vocab_size, encoding_embedding_size)`.\n",
    "- Process target data using your `process_decoder_input(target_data, target_vocab_to_int, batch_size)` function.\n",
    "- Decode the encoded input using your `decoding_layer(dec_input, enc_state, target_sequence_length, max_target_sentence_length, rnn_size, num_layers, target_vocab_to_int, target_vocab_size, batch_size, keep_prob, dec_embedding_size)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    \"\"\"\n",
    "    Build the Sequence-to-Sequence part of the neural network\n",
    "    :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n",
    "    \"\"\"\n",
    "    _, encoder_state = encoding_layer(input_data, rnn_size, num_layers, keep_prob, \n",
    "                                       source_sequence_length, source_vocab_size, \n",
    "                                       enc_embedding_size)\n",
    "\n",
    "    dec_input = process_decoder_input(target_data, target_vocab_to_int, batch_size)\n",
    "    \n",
    "    train_outputs, infer_outputs = decoding_layer(dec_input, encoder_state,\n",
    "                                                   target_sequence_length, max_target_sentence_length,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers, target_vocab_to_int, target_vocab_size,\n",
    "                                                   batch_size, keep_prob, dec_embedding_size)\n",
    "    \n",
    "    return train_outputs, infer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `num_layers` to the number of layers.\n",
    "- Set `encoding_embedding_size` to the size of the embedding for the encoder.\n",
    "- Set `decoding_embedding_size` to the size of the embedding for the decoder.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `keep_probability` to the Dropout keep probability\n",
    "- Set `display_step` to state how many steps between each debug output statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 128\n",
    "# RNN Size\n",
    "rnn_size = 64\n",
    "# Number of Layers\n",
    "num_layers = 2\n",
    "# Embedding Size\n",
    "encoding_embedding_size = 64\n",
    "decoding_embedding_size = 64\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Dropout Keep Probability\n",
    "keep_probability = 0.8\n",
    "display_step = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_path = 'checkpoints/dev'\n",
    "(source_int_text, target_int_text), (source_vocab_to_int, target_vocab_to_int), _ = util.load_preprocess()\n",
    "max_target_sentence_length = max([len(sentence) for sentence in target_int_text])\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    input_data, targets, lr, keep_prob, target_sequence_length, max_target_sequence_length, source_sequence_length, global_step = model_inputs()\n",
    "\n",
    "    input_shape = tf.shape(input_data)\n",
    "\n",
    "    train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int)\n",
    "\n",
    "    # make a copy of train_logits\n",
    "    training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "    # make a copy of inference_logits\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "\n",
    "    # masks are created to facilitate the calculation of the loss.\n",
    "    # We don't want to calculate the loss for the padding in a sentence.\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping is applied to mitigate the issue of exploding gradients\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training, validation, and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for test data 1379,1379\n"
     ]
    }
   ],
   "source": [
    "# split original data into training and test data \n",
    "source_train, source_test, target_train, target_test = train_test_split(source_int_text, \n",
    "                                                                        target_int_text, test_size=0.01, random_state=42)\n",
    "\n",
    "print(\"The size for test data {},{}\".format(len(source_test), len(target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch and pad the source and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size for training data 136354,136354\n",
      "The size for vallidation data 128,128\n"
     ]
    }
   ],
   "source": [
    "# Split data to training and validation sets\n",
    "train_source = source_train[batch_size:]\n",
    "train_target = target_train[batch_size:]\n",
    "valid_source = source_train[:batch_size]\n",
    "valid_target = target_train[:batch_size]\n",
    "(valid_sources_batch, valid_targets_batch, valid_sources_lengths, valid_targets_lengths ) = next(util.get_batches(valid_source,\n",
    "                                                                                                             valid_target,\n",
    "                                                                                                             batch_size,\n",
    "                                                                                                             source_vocab_to_int['<PAD>'],\n",
    "                                                                                                             target_vocab_to_int['<PAD>']))\n",
    "\n",
    "print(\"The size for training data {},{}\".format(len(train_source), len(train_target)))\n",
    "print(\"The size for vallidation data {},{}\".format(len(valid_source), len(valid_target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Train the neural network on the preprocessed data. If you have a hard time getting a good loss, check the forms to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1000/1077 - Loss: 0.8160\n",
      "Epoch   1 Batch 1000/1077 - Loss: 0.5620\n",
      "Epoch   2 Batch 1000/1077 - Loss: 0.4408\n",
      "Epoch   3 Batch 1000/1077 - Loss: 0.3253\n",
      "Epoch   4 Batch 1000/1077 - Loss: 0.2479\n",
      "Epoch   5 Batch 1000/1077 - Loss: 0.1898\n",
      "Epoch   6 Batch 1000/1077 - Loss: 0.1620\n",
      "Epoch   7 Batch 1000/1077 - Loss: 0.1223\n",
      "Epoch   8 Batch 1000/1077 - Loss: 0.0955\n",
      "Epoch   9 Batch 1000/1077 - Loss: 0.0808\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "                                                                                             \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                util.get_batches(train_source, train_target, batch_size,\n",
    "                                source_vocab_to_int['<PAD>'],\n",
    "                                target_vocab_to_int['<PAD>'])):\n",
    "            count += 1 \n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: source_batch,\n",
    "                 targets: target_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "            \n",
    "            loss_list.append((count, loss))\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "\n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(source_int_text) // batch_size,  loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the training loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21d11eedb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(*zip(*loss_list), label=\"Train Loss\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Parameters\n",
    "Save the `batch_size` and `save_path` parameters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "util.save_params(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "_, (source_vocab_to_int, target_vocab_to_int), (source_int_to_vocab, target_int_to_vocab) = util.load_preprocess()\n",
    "load_path = util.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model\n",
    "Finally, the trained neural machine translation model is evaluated using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/dev\n",
      "Input\n",
      "  Word Ids:      [127, 116, 218, 174, 59, 20, 99, 154, 23, 116, 218, 187, 223, 221, 216, 0, 0]\n",
      "  English Words: california is sometimes dry during may , and it is sometimes wonderful in february .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [78, 328, 253, 121, 173, 161, 141, 128, 315, 157, 14, 328, 253, 219, 38, 210, 170]\n",
      "  French Words: californie est parfois sec au mois de mai , et il est parfois merveilleux en février .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [78, 328, 253, 121, 173, 161, 141, 128, 315, 157, 14, 328, 253, 219, 38, 210, 170, 1, 0, 0, 0]\n",
      "  French Words: californie est parfois sec au mois de mai , et il est parfois merveilleux en février .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [14, 116, 218, 184, 59, 183, 99, 154, 23, 116, 173, 181, 223, 103, 216, 0, 0]\n",
      "  English Words: paris is sometimes beautiful during summer , and it is usually snowy in fall .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [10, 328, 253, 40, 108, 300, 288, 315, 157, 14, 328, 232, 30, 265, 300, 12, 170]\n",
      "  French Words: paris est parfois belle pendant l' été , et il est généralement enneigée à l' automne .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [10, 328, 253, 40, 108, 300, 288, 315, 157, 14, 328, 232, 30, 265, 300, 12, 170, 1, 0, 0]\n",
      "  French Words: paris est parfois belle pendant l' été , et il est généralement enneigée à l' automne .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [225, 194, 82, 33, 116, 63, 158, 99, 219, 80, 194, 82, 116, 63, 74, 216, 0]\n",
      "  English Words: your least liked fruit is the pear , but their least liked is the banana .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [295, 301, 305, 23, 328, 113, 189, 315, 142, 139, 301, 305, 328, 113, 249, 170, 1]\n",
      "  French Words: votre moins aimé fruit est la poire , mais leur moins aimé est la banane .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [295, 301, 305, 23, 328, 113, 189, 315, 142, 139, 301, 305, 328, 113, 249, 170, 1, 0, 0, 0]\n",
      "  French Words: votre moins aimé fruit est la poire , mais leur moins aimé est la banane .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [19, 97, 116, 203, 59, 144, 99, 219, 23, 116, 218, 30, 223, 103, 216, 0, 0]\n",
      "  English Words: new jersey is quiet during january , but it is sometimes rainy in fall .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [103, 224, 328, 241, 38, 340, 315, 142, 14, 328, 253, 36, 265, 300, 12, 170, 1]\n",
      "  French Words: new jersey est calme en janvier , mais il est parfois pluvieux à l' automne .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [103, 224, 328, 241, 38, 340, 315, 142, 14, 328, 253, 36, 265, 300, 12, 170, 1, 0, 0, 0]\n",
      "  French Words: new jersey est calme en janvier , mais il est parfois pluvieux à l' automne .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [29, 116, 182, 174, 59, 27, 99, 219, 23, 116, 173, 24, 223, 221, 216, 0, 0]\n",
      "  English Words: china is never dry during march , but it is usually mild in february .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [52, 328, 222, 265, 121, 38, 46, 315, 142, 14, 328, 232, 8, 38, 210, 170, 1]\n",
      "  French Words: chine est jamais à sec en mars , mais il est généralement doux en février .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [52, 328, 222, 265, 121, 173, 161, 141, 46, 315, 142, 14, 328, 232, 8, 38, 210, 170, 1, 0]\n",
      "  French Words: chine est jamais à sec au mois de mars , mais il est généralement doux en février .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [127, 116, 173, 61, 59, 51, 99, 219, 23, 116, 182, 30, 223, 144, 216, 0, 0]\n",
      "  English Words: california is usually busy during spring , but it is never rainy in january .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [78, 328, 232, 296, 173, 302, 315, 142, 222, 120, 278, 38, 340, 170, 1, 0, 0]\n",
      "  French Words: californie est généralement occupé au printemps , mais jamais des pluies en janvier .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [78, 328, 232, 296, 173, 302, 315, 142, 222, 120, 278, 38, 340, 170, 1, 0, 0, 0, 0, 0]\n",
      "  French Words: californie est généralement occupé au printemps , mais jamais des pluies en janvier .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [63, 70, 116, 57, 159, 136, 33, 99, 219, 63, 209, 116, 54, 159, 136, 216, 0]\n",
      "  English Words: the apple is my most loved fruit , but the grape is his most loved .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [113, 75, 328, 191, 23, 191, 56, 305, 212, 315, 142, 191, 335, 328, 191, 56, 305]\n",
      "  French Words: la pomme est le fruit le plus aimé notre , mais le raisin est le plus aimé\n",
      "\n",
      "Target\n",
      "  Word Ids:      [113, 75, 328, 191, 23, 191, 56, 207, 276, 315, 142, 191, 335, 328, 191, 56, 305, 170, 1]\n",
      "  French Words: la pomme est le fruit le plus mon cher , mais le raisin est le plus aimé .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [143, 76, 138, 99, 45, 99, 154, 126, 216, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  English Words: i dislike pears , grapefruit , and bananas .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [162, 321, 82, 187, 163, 315, 187, 251, 157, 187, 57, 170, 1, 0, 0, 0, 0]\n",
      "  French Words: je n'aime pas les poires , les pamplemousses et les bananes .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [162, 321, 82, 187, 163, 315, 187, 251, 157, 187, 57, 170, 1, 0, 0, 0, 0, 0, 0]\n",
      "  French Words: je n'aime pas les poires , les pamplemousses et les bananes .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [204, 90, 83, 99, 41, 99, 154, 145, 216, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  English Words: he likes mangoes , grapes , and lemons .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [14, 77, 187, 206, 315, 187, 81, 157, 187, 35, 170, 1, 0, 0, 0, 0, 0]\n",
      "  French Words: il aime les mangues , les raisins et les citrons .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [14, 77, 187, 206, 315, 187, 81, 157, 187, 35, 170, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  French Words: il aime les mangues , les raisins et les citrons .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Input\n",
      "  Word Ids:      [190, 116, 182, 184, 59, 20, 99, 219, 23, 116, 150, 223, 230, 216, 0, 0, 0]\n",
      "  English Words: france is never beautiful during may , but it is hot in august .\n",
      "\n",
      "Prediction\n",
      "  Word Ids:      [113, 165, 328, 222, 40, 173, 161, 141, 128, 315, 142, 14, 328, 244, 38, 80, 170]\n",
      "  French Words: la france est jamais belle au mois de mai , mais il est chaud en août .\n",
      "\n",
      "Target\n",
      "  Word Ids:      [113, 165, 328, 222, 40, 173, 161, 141, 128, 315, 142, 14, 328, 244, 38, 80, 170, 1, 0, 0]\n",
      "  French Words: la france est jamais belle au mois de mai , mais il est chaud en août .\n",
      "\n",
      "------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_path + '.meta')\n",
    "    loader.restore(sess, load_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "    source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "                util.get_batches(source_test, target_test, batch_size,\n",
    "                                source_vocab_to_int['<PAD>'],\n",
    "                                target_vocab_to_int['<PAD>'])):\n",
    "        \n",
    "        test_logits = sess.run(logits, {input_data: source_batch,\n",
    "                               target_sequence_length: sources_lengths,\n",
    "                               source_sequence_length: targets_lengths,\n",
    "                               keep_prob: 1.0})\n",
    "        \n",
    "        print('Input')\n",
    "        print('  Word Ids:      {}'.format([i for i in source_batch[0]]))\n",
    "        print('  English Words: {}'.format(\" \".join([source_int_to_vocab[i] for i in source_batch[0] \n",
    "                                           if i not in [0, 1]])))\n",
    "\n",
    "        print('\\nPrediction')\n",
    "        print('  Word Ids:      {}'.format([i for i in test_logits[0]]))\n",
    "        print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in test_logits[0] \n",
    "                                                   if i not in [0, 1]])))\n",
    "        \n",
    "        print('\\nTarget')\n",
    "        print('  Word Ids:      {}'.format([i for i in target_batch[0]]))\n",
    "        print('  French Words: {}'.format(\" \".join([target_int_to_vocab[i] for i in target_batch[0]\n",
    "                                                  if i not in [0, 1]])))\n",
    "        print(\"\\n------------------------------------------------------------------\\n\")\n",
    "        \n",
    "        test_acc = util.get_bleu(target_batch, test_logits)\n",
    "        test_acc_list.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU score for the test data is 0.797999510477571\n"
     ]
    }
   ],
   "source": [
    "print(\"The BLEU score for the test data is {}\".format(np.mean(test_acc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
